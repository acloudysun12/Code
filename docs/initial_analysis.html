<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Adam Sun" />

<meta name="date" content="2020-02-01" />

<title>Initial Analysis</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Code</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/acloudysun12/Code">
    <span class="fa fa-github"></span>
     
    Source code
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Initial Analysis</h1>
<h4 class="author"><em>Adam Sun</em></h4>
<h4 class="date"><em>2020-02-01</em></h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2020-02-17
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>Code/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.4.0). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date </a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate" class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git repository, you know the exact version of the code that produced these results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20191013code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20191013)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20191013code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20191013)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomacloudysun12Codetree9892c8f4e4a1c3fa076193d0884316923499813etargetblank9892c8fa"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/acloudysun12/Code/tree/9892c8f4e4a1c3fa076193d0884316923499813e" target="_blank">9892c8f</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomacloudysun12Codetree9892c8f4e4a1c3fa076193d0884316923499813etargetblank9892c8fa" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility. The version displayed above was the version of the Git repository at the time these results were generated. <br><br> Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rhistory
    Ignored:    analysis/.Rhistory
    Ignored:    code/sn_spMF/.Rhistory

Untracked files:
    Untracked:  data/W_sim.txt
    Untracked:  data/X_sim.txt

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the R Markdown and HTML files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view them.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/acloudysun12/Code/blob/9892c8f4e4a1c3fa076193d0884316923499813e/analysis/initial_analysis.Rmd" target="_blank">9892c8f</a>
</td>
<td>
Adam Sun
</td>
<td>
2020-02-17
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/acloudysun12/Code/blob/0155c86ad7dafaa7447718cabf6737d898a69b97/analysis/initial_analysis.Rmd" target="_blank">0155c86</a>
</td>
<td>
Adam Sun
</td>
<td>
2020-02-17
</td>
<td>
First sim results, 1fac and 5facs matrices
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/acloudysun12/Code/ab998e5ce996bed55e3c481d4396dfac549dd783/docs/initial_analysis.html" target="_blank">ab998e5</a>
</td>
<td>
Adam Sun
</td>
<td>
2020-02-17
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/acloudysun12/Code/blob/1d506b922f2c084fde9bef1c928284139e02c645/analysis/initial_analysis.Rmd" target="_blank">1d506b9</a>
</td>
<td>
Adam Sun
</td>
<td>
2020-02-17
</td>
<td>
Initial analysis with 1 fac + 5 fac matrices
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/acloudysun12/Code/blob/8fbd2006769e9ca6d2d5cb95abf0810055ec454b/analysis/initial_analysis.Rmd" target="_blank">8fbd200</a>
</td>
<td>
Adam Sun
</td>
<td>
2020-02-17
</td>
<td>
Initial publish for 1 fac, 5 facs matrix factorization
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/acloudysun12/Code/blob/5b56f69bcd77417952f5c9afd7c93ed077e52b76/analysis/initial_analysis.Rmd" target="_blank">5b56f69</a>
</td>
<td>
Adam Sun
</td>
<td>
2020-02-17
</td>
<td>
initial analysis with 1 factor, 5 factor matrices
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/acloudysun12/Code/blob/0ccbff44c4d324ace062ae952c8572aa0c114d75/analysis/initial_analysis.Rmd" target="_blank">0ccbff4</a>
</td>
<td>
Adam Sun
</td>
<td>
2020-02-17
</td>
<td>
Files needed for running sn-spmf
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/acloudysun12/Code/0ccbff44c4d324ace062ae952c8572aa0c114d75/docs/initial_analysis.html" target="_blank">0ccbff4</a>
</td>
<td>
Adam Sun
</td>
<td>
2020-02-17
</td>
<td>
Files needed for running sn-spmf
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/acloudysun12/Code/590c4f17e01603cca6c4bf1fb2282f53463d6ce6/docs/initial_analysis.html" target="_blank">590c4f1</a>
</td>
<td>
Adam Sun
</td>
<td>
2019-11-04
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/acloudysun12/Code/blob/47a51894ca970a70019aa8e7e85271cf852748b3/analysis/initial_analysis.Rmd" target="_blank">47a5189</a>
</td>
<td>
Adam Sun
</td>
<td>
2019-11-04
</td>
<td>
flashr 1-D, non-negative factor
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/acloudysun12/Code/4d5fa7028d82ec1106b461576906c2f91c44cd90/docs/initial_analysis.html" target="_blank">4d5fa70</a>
</td>
<td>
Adam Sun
</td>
<td>
2019-10-13
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/acloudysun12/Code/blob/1ab856d29ceebbcccb7b0d5c7a63e4f07ea8b017/analysis/initial_analysis.Rmd" target="_blank">1ab856d</a>
</td>
<td>
Adam Sun
</td>
<td>
2019-10-13
</td>
<td>
First analysis
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="notes" class="section level3">
<h3>Notes:</h3>
<p>Yuan He’s matrix factorization algorithm (sn-spMF) solves for non-negative F because “penalized”, the function used for iteratively solving for L and F, includes parameter to constrain coefficients to non-negative values. See fit_F.R, “penalized(…positive=T)”.</p>
<p>K, the number of factors, is selected outside the matrix factorization algorithm. Paper references producing a consensus matrix C “…after 30 runs with random initialization” and selecting K based off on which K maximizes the cophenetic correlation (uses compute_cophenet.R).</p>
<p>Sparsity coefficients on loadings matrix (L) and factors matrix (F)–denoted alpha and lambda in paper–are selected after K is selected.</p>
<p>NOTE – not sure how to make website reference not hard-coded.</p>
<pre><code>Warning: package &#39;penalized&#39; was built under R version 3.5.3</code></pre>
<pre><code>Loading required package: survival</code></pre>
<pre><code>Welcome to penalized. For extended examples, see vignette(&quot;penalized&quot;).</code></pre>
<pre><code>Warning: package &#39;optparse&#39; was built under R version 3.5.3</code></pre>
<pre><code>-- Attaching packages ------------------------------------------------------------------ tidyverse 1.2.1 --</code></pre>
<pre><code>v ggplot2 3.2.1       v purrr   0.3.2  
v tibble  2.1.3       v dplyr   0.8.0.1
v tidyr   0.8.1       v stringr 1.4.0  
v readr   1.1.1       v forcats 0.3.0  </code></pre>
<pre><code>Warning: package &#39;ggplot2&#39; was built under R version 3.5.3</code></pre>
<pre><code>Warning: package &#39;tibble&#39; was built under R version 3.5.3</code></pre>
<pre><code>Warning: package &#39;purrr&#39; was built under R version 3.5.3</code></pre>
<pre><code>Warning: package &#39;dplyr&#39; was built under R version 3.5.3</code></pre>
<pre><code>Warning: package &#39;stringr&#39; was built under R version 3.5.3</code></pre>
<pre><code>-- Conflicts --------------------------------------------------------------------- tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()</code></pre>
<pre><code>Warning: package &#39;devtools&#39; was built under R version 3.5.3</code></pre>
<pre><code>Loading required package: usethis</code></pre>
<pre><code>Warning: package &#39;usethis&#39; was built under R version 3.5.3</code></pre>
<pre><code>Warning: package &#39;NNLM&#39; was built under R version 3.5.3</code></pre>
<pre class="r"><code>inputdir = &quot;https://raw.githubusercontent.com/acloudysun12/Matrix_Facs/master/code/sn_spMF/&quot;
source(paste0(inputdir, &quot;compute_obj.R&quot;))
source(paste0(inputdir, &quot;fit_L.R&quot;))
source(paste0(inputdir, &quot;fit_F.R&quot;))
source(paste0(inputdir, &quot;readIn.R&quot;))
source(paste0(inputdir, &quot;Update_FL_AS.R&quot;))
source(paste0(inputdir, &quot;compute_cophenet.R&quot;))</code></pre>
<pre><code>-------------------------------------------------------------------------</code></pre>
<pre><code>You have loaded plyr after dplyr - this is likely to cause problems.
If you need functions from both plyr and dplyr, please load plyr first, then dplyr:
library(plyr); library(dplyr)</code></pre>
<pre><code>-------------------------------------------------------------------------</code></pre>
<pre><code>
Attaching package: &#39;plyr&#39;</code></pre>
<pre><code>The following objects are masked from &#39;package:dplyr&#39;:

    arrange, count, desc, failwith, id, mutate, rename, summarise,
    summarize</code></pre>
<pre><code>The following object is masked from &#39;package:purrr&#39;:

    compact</code></pre>
<pre class="r"><code>run_nnf = 
function(X_mtx, W_mtx, num_facs, 
         max_iters = 100, 
         penalty_L = 0.1, 
         penalty_F = 0.1, 
         option_disp = T){
  

  # Define hyperparams for test
  K = num_facs
  alpha1 = penalty_L
  lambda1 = penalty_F
  xfn=X_mtx %&gt;% as.data.frame()
  wfn=W_mtx %&gt;% as.data.frame()
  Data = readIn(K, alpha1, lambda1, xfn, wfn) 
  X = Data[[&#39;X&#39;]];
  W = Data[[&#39;W&#39;]];
  option = Data[[&#39;option&#39;]];
  option[[&#39;iter&#39;]]  = max_iters;
  option[[&#39;disp&#39;]]  = option_disp
  Fn_basename = Data[[&#39;Fn_basename&#39;]];
  
  ## run MF to learn the factors  
  # print(paste0(&#39;Initial K=&#39;, (K), &#39;; alpha1=&#39;, round(alpha1, 3),&#39;; lambda1=&#39;, round(lambda1, 3)));

  Run_iter = Update_FL(X, W, option);
  FactorM = Run_iter[[1]]
  LoadingM = Run_iter[[2]]
  factor_corr = norm(cor(FactorM), &#39;F&#39;)
  L_sparsity = Run_iter[[3]]
  F_sparsity = Run_iter[[4]]
  print(paste0(&#39;Final sparsity in Factor matrix =&#39;, (F_sparsity),&#39;; Final sparsity in L =&#39;, (L_sparsity), &#39;; &#39;))
  
  return(list(FactorM = FactorM, LoadingM = LoadingM))
}</code></pre>
<p><br> <br></p>
</div>
<div id="first-simulation" class="section level2">
<h2>First Simulation</h2>
<p><br></p>
<div id="simulated-matrix" class="section level3">
<h3>Simulated matrix</h3>
<p>Understand conceptually. Simulate 200x10 matrix. 50% sparse on factors (F) and 50% sparse on loadings (L). We set F to be non-negative with exp(1/2) distribution. Set to exp(1/2) so that signal is stronger than noise (SD of Error matrix = 1/2). L has N(0,1) distribution.</p>
<p>Use RRMSE for measurement (as per Wang Stephens paper) for comparison of methods.</p>
<pre class="r"><code>set.seed(10000)

F_sparse = 0.50 # the true percentage of 0&#39;s
L_sparse = 0.50 # the true percentage of 0&#39;s


i = 1
# matrix prep
N = 200
P = 10
sd_noise = 0.5
sparseF_sim = 1 - rbinom(P, 1, prob = F_sparse[i]) 

F_sim = ifelse(sparseF_sim == 0, 0, rexp(P, rate = 1/2)) %&gt;% matrix(nrow = P) 
sparseL_sim = 1 - rbinom(N, 1, prob = L_sparse[i])
L_sim = ifelse(sparseL_sim == 0, 0, rnorm(N, 0, 1)) %&gt;% matrix() # (nx1)
E_sim = rnorm(n = N*P, 0, sd_noise) %&gt;% matrix(nrow = N)
X_sim = L_sim%*%t(F_sim) + E_sim
X_true = L_sim%*%t(F_sim)

F_sparse_true = sum(F_sim ==0)/length(F_sim)
paste0(&quot;F true sparsity: &quot;, F_sparse_true)</code></pre>
<pre><code>[1] &quot;F true sparsity: 0.4&quot;</code></pre>
<pre class="r"><code>L_sparse_true = sum(L_sim ==0)/length(L_sim)
paste0(&quot;L true sparsity: &quot;, L_sparse_true)</code></pre>
<pre><code>[1] &quot;L true sparsity: 0.535&quot;</code></pre>
<p><br></p>
</div>
<div id="sn-spmf" class="section level3">
<h3>sn-spMF</h3>
<p>Edited nnf code so it’s compatible with 1 factor.</p>
<p>Ideally, we would choose a vector of factors K and sparsity parameter values for L and F. However, the method of cophenetic correlation calculation (used in sn-spmf for finding the optimal number of factors K does not apply when K = 1). So for this simulation, we suppose that K = 1 is known. However, we run with initial K = 1 and 2 to see if sn-spmf redcues K to 1, We also vary sparsity parameters for L and F. We compute RRMSE from all runs to compare against EBMF method.</p>
<p>Note: sparsity parameter on Loadings (alpha1) could cause all elements shrunk to zero. Set max to 1.</p>
<pre><code>[1] &quot;K = 1 , F Penalty = 0.067&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.491747 secs
[1] &quot;Final sparsity in Factor matrix =0; Final sparsity in L =0.05; &quot;
[1] &quot;K = 1 , F Penalty = 0.2735&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.168571 secs
[1] &quot;Final sparsity in Factor matrix =0; Final sparsity in L =0.1; &quot;
[1] &quot;K = 1 , F Penalty = 0.1793&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.713286 secs
[1] &quot;Final sparsity in Factor matrix =0; Final sparsity in L =0.095; &quot;
[1] &quot;K = 1 , F Penalty = 0.0062&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 4.379638 secs
[1] &quot;Final sparsity in Factor matrix =0; Final sparsity in L =0.05; &quot;
[1] &quot;K = 1 , F Penalty = 0.3835&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.394374 secs
[1] &quot;Final sparsity in Factor matrix =0; Final sparsity in L =0.05; &quot;
[1] &quot;K = 1 , F Penalty = 0.8655&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.49209 secs
[1] &quot;Final sparsity in Factor matrix =0; Final sparsity in L =0.08; &quot;
[1] &quot;K = 1 , F Penalty = 0.6999&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.244857 secs
[1] &quot;Final sparsity in Factor matrix =0; Final sparsity in L =0.06; &quot;
[1] &quot;K = 1 , F Penalty = 1.6011&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.297573 secs
[1] &quot;Final sparsity in Factor matrix =0.1; Final sparsity in L =0.06; &quot;
[1] &quot;K = 1 , F Penalty = 0.9802&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.319346 secs
[1] &quot;Final sparsity in Factor matrix =0; Final sparsity in L =0.12; &quot;
[1] &quot;K = 1 , F Penalty = 1.5402&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.378473 secs
[1] &quot;Final sparsity in Factor matrix =0.1; Final sparsity in L =0.025; &quot;
[1] &quot;K = 1 , F Penalty = 2.8994&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.322274 secs
[1] &quot;Final sparsity in Factor matrix =0.2; Final sparsity in L =0; &quot;
[1] &quot;K = 1 , F Penalty = 2.9647&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.254742 secs
[1] &quot;Final sparsity in Factor matrix =0.2; Final sparsity in L =0.055; &quot;
[1] &quot;K = 1 , F Penalty = 4.7969&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.260832 secs
[1] &quot;Final sparsity in Factor matrix =0.2; Final sparsity in L =0.02; &quot;
[1] &quot;K = 1 , F Penalty = 4.1581&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.256818 secs
[1] &quot;Final sparsity in Factor matrix =0.2; Final sparsity in L =0.06; &quot;
[1] &quot;K = 1 , F Penalty = 4.834&quot;
[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.201594 secs
[1] &quot;Final sparsity in Factor matrix =0.2; Final sparsity in L =0.05; &quot;
[1] &quot;K = 2 , F Penalty = 0.4195&quot;
[1] &quot;Converged at itertaion 6&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 9.047909 secs
[1] &quot;Final sparsity in Factor matrix =0.3; Final sparsity in L =0.1125; &quot;
[1] &quot;K = 2 , F Penalty = 0.2668&quot;
[1] &quot;Converged at itertaion 5&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 7.771375 secs
[1] &quot;Final sparsity in Factor matrix =0.2; Final sparsity in L =0.0425; &quot;
[1] &quot;K = 2 , F Penalty = 0.0938&quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 1.084566 mins
[1] &quot;Final sparsity in Factor matrix =0.45; Final sparsity in L =0.07; &quot;
[1] &quot;K = 2 , F Penalty = 0.2086&quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 1.077364 mins
[1] &quot;Final sparsity in Factor matrix =0.4; Final sparsity in L =0.0475; &quot;
[1] &quot;K = 2 , F Penalty = 0.1579&quot;
[1] &quot;Converged at itertaion 22&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 29.08072 secs
[1] &quot;Final sparsity in Factor matrix =0.4; Final sparsity in L =0.0475; &quot;
[1] &quot;K = 2 , F Penalty = 0.6271&quot;
[1] &quot;Converged at itertaion 26&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 33.793 secs
[1] &quot;Final sparsity in Factor matrix =0.45; Final sparsity in L =0.1125; &quot;
[1] &quot;K = 2 , F Penalty = 1.6884&quot;
[1] &quot;Converged at itertaion 7&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 10.08334 secs
[1] &quot;Final sparsity in Factor matrix =0.35; Final sparsity in L =0.055; &quot;
[1] &quot;K = 2 , F Penalty = 0.8019&quot;
[1] &quot;Converged at itertaion 29&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 38.03485 secs
[1] &quot;Final sparsity in Factor matrix =0.45; Final sparsity in L =0.0675; &quot;
[1] &quot;K = 2 , F Penalty = 0.9453&quot;
[1] &quot;Converged at itertaion 21&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 26.73593 secs
[1] &quot;Final sparsity in Factor matrix =0.45; Final sparsity in L =0.13; &quot;
[1] &quot;K = 2 , F Penalty = 1.3803&quot;
[1] &quot;Converged at itertaion 19&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 24.52623 secs
[1] &quot;Final sparsity in Factor matrix =0.5; Final sparsity in L =0.1475; &quot;
[1] &quot;K = 2 , F Penalty = 4.7249&quot;
[1] &quot;Converged at itertaion 3&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 4.395304 secs
[1] &quot;Final sparsity in Factor matrix =0.2; Final sparsity in L =0.13; &quot;
[1] &quot;K = 2 , F Penalty = 3.7965&quot;
[1] &quot;Converged at itertaion 38&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 49.93339 secs
[1] &quot;Final sparsity in Factor matrix =0.55; Final sparsity in L =0.0525; &quot;
[1] &quot;K = 2 , F Penalty = 2.6524&quot;
[1] &quot;Converged at itertaion 14&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 18.31466 secs
[1] &quot;Final sparsity in Factor matrix =0.55; Final sparsity in L =0.1875; &quot;
[1] &quot;K = 2 , F Penalty = 2.8155&quot;
[1] &quot;Converged at itertaion 14&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 19.39279 secs
[1] &quot;Final sparsity in Factor matrix =0.5; Final sparsity in L =0.1275; &quot;
[1] &quot;K = 2 , F Penalty = 4.1009&quot;
[1] &quot;Converged at itertaion 7&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 10.68712 secs
[1] &quot;Final sparsity in Factor matrix =0.45; Final sparsity in L =0.045; &quot;</code></pre>
<p><br></p>
</div>
<div id="ebmf-with-flashr" class="section level3">
<h3>EBMF with flashr</h3>
<p>Next, perform matrix factorization with EBMF. We use non-negative mf proposed by JW. We change the “ebnm_param” object so that F is non-negative but L can be any number. We do this by assigning “uniform+” prior on F, and a default normal prior with mean 0 and 1 SD to L.</p>
<p>Reference: <a href="https://willwerscheid.github.io/FLASHvestigations/nonnegative.html" class="uri">https://willwerscheid.github.io/FLASHvestigations/nonnegative.html</a></p>
<p>We first use the normal flash function to confirm that L takes any values and F takes non-negative values. Next, we run flash_backfit over a range of starting K’s from 1 to 5 and choose the final run as the one which maximizes the objective.</p>
<pre class="r"><code># flash object of simulated X matrix
X_flash = flash_set_data(X_sim)

ebnm_param_l = list(g=ashr::normalmix(1,0,1), fixg=TRUE)
ebnm_fn = &quot;ebnm_ash&quot;
ebnm_param = list(
                  l = ebnm_param_l, # L can be any number
                  f = list(mixcompdist = &quot;+uniform&quot;), # Factors are non-negative
                  warmstart = TRUE)

# non-negative 
udv_nn = function(Y, K = 1) {
  tmp = NNLM::nnmf(Y, K, verbose = FALSE)
  return(list(d = rep(1, K), u = tmp$W, v = t(tmp$H)))
}

# function to run flash once (for any matrix X)
run_flash_once &lt;- function(X_mtx, f_init) {
  flash(X_mtx, f_init = f_init, 
        ebnm_fn = ebnm_fn, ebnm_param = ebnm_param,
        var_type=&quot;constant&quot;, init_fn = udv_nn, 
        backfit = TRUE, verbose = FALSE)
}


fl_init &lt;- flash_add_factors_from_data(X_flash, K = 5,
                                       init_fn = udv_nn,
                                       ebnm_param = ebnm_param, 
                                       backfit = FALSE)
fl_iter &lt;- run_flash_once(X_sim, f_init = fl_init)
print(paste0(&quot;number of non-negative elements in l: &quot;, length(which(fl_iter$ldf$l &lt; 0)))) </code></pre>
<pre><code>[1] &quot;number of non-negative elements in l: 96&quot;</code></pre>
<pre class="r"><code>print(paste0(&quot;number of non-negative elements in f: &quot;, length(which(fl_iter$ldf$f &lt; 0)))) </code></pre>
<pre><code>[1] &quot;number of non-negative elements in f: 0&quot;</code></pre>
<pre class="r"><code>df_ebmf_results = matrix(0, nrow = 0, ncol = 5) %&gt;% as.data.frame()
all_flash_f = list()
all_flash_l = list()
all_flash_d = list()

for (K_iter in 1:5){
  fl_init &lt;- flash_add_factors_from_data(X_flash, K = K_iter,
                                       init_fn = udv_nn,
                                       ebnm_param = ebnm_param, 
                                       backfit = FALSE)
  time_eb = Sys.time()
  fl_back &lt;- flash_backfit(data = X_flash, f_init = fl_init,
                           ebnm_fn = ebnm_fn,
                           ebnm_param = ebnm_param,
                           var_type = &quot;constant&quot;,
                           verbose = FALSE)
  
  fl_back_l = fl_back$ldf$l
  fl_back_d = diag(x = fl_back$ldf$d, nrow = length(fl_back$ldf$d))
  fl_back_f = fl_back$ldf$f
  
  X_hat_eb = fl_back_l %*% fl_back_d %*% t(fl_back_f)
  RRMSE_eb = sqrt(sum((X_hat_eb - X_true)^2)/sum(X_true^2))
  time_eb = difftime(Sys.time(), time_eb, units = &quot;secs&quot;)
  df_ebmf_results = rbind(df_ebmf_results, 
                    c(K_iter, time_eb, fl_back$nfactors, fl_back$objective, RRMSE_eb))
  all_flash_f[[K_iter]] = fl_back_f
  all_flash_l[[K_iter]] = fl_back_l
  all_flash_d[[K_iter]] = fl_back_d
  
}

colnames(df_ebmf_results) = c(&quot;K&quot;, &quot;Time_Taken&quot;, &quot;Num Factors&quot;, &quot;Objective&quot;, &quot;RRMSE&quot;)

plot(fl_back, plot_loadings = TRUE)</code></pre>
<pre><code>Warning in plot.flash(fl_back, plot_loadings = TRUE): Not enough factors to
create a scree plot.</code></pre>
<p><img src="figure/initial_analysis.Rmd/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="results-from-both-methods" class="section level3">
<h3>Results from both methods</h3>
<p>The average accuracy across all sp-snmf runs was 0.114, while ebmf produced the same RRMSE of 0.1 for all initial starting K’s. This is marginally higher than the average sp-snmf RRMSE by 0.014.</p>
<p>As mentioned before, sn-spmf chooses the best K by minimizing cophenetic correlation. However, this does not apply for K = 1, so we assume that K = 1 is known for sn-spmf and focus only on runs where final K = 1 to compare against ebmf. In this case, the average RRMSE of sn-spmf runs had an RRMSE of 0.094, and the highest accuracy (‘optimal’) run had the lowest RRMSE of 0.088. <br></p>
<p>All ebmf runs correctly identified that the true factor matrix had rank K = 1. Meanwhile, note that not all sn-spmf runs correctly identified that K = 1. When initialized with K = 2, 14 out of 15 runs did not correctly identify the true K to be 1. That, however, could be partially because the maximum iteration is set to 50. If we ran to convergence, we may perhaps see better accuracy results (at the expense of longer runs). <br></p>
<p>In terms of time, the ebmf runs outperformed sn-spmf at an individual run level. Each run of sn-spmf took an average time of 15.431 seconds. Meanwhile, ebmf runs took an average of 7.188 seconds. Additionally, when the number of factors is misspecified, sn-spmf took considerably longer (29.082 seconds when mis-specified versus 3.487 seconds when not). Meanwhile, for ebmf, for any initial K flash_backfit took relatively more similar amounts of time, ranging from 3 seconds to ~15 seconds (scales linearly with the initial number of K’s for flash_backfit, from 1 to 5). <br></p>
<p>Looking at final sparsities on L and F, the sparsities on L and F from sn-spmf – 0.13 and 0.2 – are not that close to the true values of 0.535 and 0.4. However, certain estimates may be noise, and observing the histogram of values of F below, we see that indeed any values smaller than 0.5 may potentially be treated as noise. Ignoring elements of F &lt; 0.1 gives us a sparsity estimate on F of 0.5, which is more similar to the true sparsity on F of 0.4.</p>
<p>Under ebmf, the prior distributions on L and F did not include the mix of point estimate densities, so we go with the approach above to estimate sparsity. The histogram of values of F and the log-abs_values of L indicate we can use reasonable cutoff values of 0.1 and -4 to get estimated sparsities of 0.5 and 0.405 respectively, for F and L. If we trust this hacky method of estimating sparsity on L and F, this suggests flash_backfit and sn-spmf are similar to each other in estimating the sparsity on F. In addition, flash_backfit is fairly accurate in estimating the sparsity of L of 0.535. <br></p>
<p>Finally, since both sn-spmf and ebmf both produce optimal matrices on F with 1 factor (i.e. K = 1), we do a side-by-side comparison of the two F vectors below with the true factor F used to generate the matrix X. While the orders of magnitude are different, the indices of elements which are larger than other elements in their respective vectors tie out accordingly across the three F’s.</p>
<p><img src="figure/initial_analysis.Rmd/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /><img src="figure/initial_analysis.Rmd/unnamed-chunk-9-2.png" width="672" style="display: block; margin: auto;" /><img src="figure/initial_analysis.Rmd/unnamed-chunk-9-3.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>  sn-spmf F flash_back F true F
f     3.210        5.169  4.336
f     2.286        3.726  3.089
f     2.768        4.504  3.726
f     0.031        0.052  0.000
f     1.096        1.805  1.497
f     0.197        0.355  0.306
f     0.007        0.041  0.000
f     0.000        0.034  0.000
f     0.000        0.036  0.000
f     0.775        1.294  1.020</code></pre>
<pre><code>[1] &quot;Converged at itertaion 2&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.207611 secs
[1] &quot;Final sparsity in Factor matrix =0.3; Final sparsity in L =0.105; &quot;</code></pre>
<p>As a thought experiment, I try to do better on sn-spmf by modeling the RRMSE as a quadratic function of the starting on L and F to find optimal solution for them . (Note I have to bound L above by 1 and F by 10, otherwise it seems the sparsity on L and F may get pushed initially to 1 and cause either matrices to be the 0 matrix). The optimal sparsity estimates on F and L is the vector <span class="math inline">\((\lambda_1, \alpha_1)\)</span> = 10, 0.65. This is not surprising since the coefficient on the quadratic term F is negative.</p>
<p>This does not achieve a better accuracy than our best sn-spmf run, resulting in an RRMSE of 0.091. The final sparsities on F and L on this run are 0.3 and 0.105.</p>
<pre><code>[1] &quot;Final &#39;optimal&#39; run results:&quot;</code></pre>
<pre><code>    Run K_start K_final F_lambda1 L_alpha1 F_sparse_final L_sparse_final
31 9999       1       1        10     0.65            0.3          0.105
       Time      Error
31 3.213612 0.09058209</code></pre>
<pre><code>[1] &quot;Reg coefficients with quadratic terms:&quot;</code></pre>
<pre><code>                   Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)           0.134      0.016   8.268    0.000
I(F_lambda1^2)       -0.001      0.002  -0.664    0.513
I(L_alpha1^2)         0.025      0.079   0.313    0.757
F_lambda1             0.002      0.010   0.197    0.845
L_alpha1             -0.060      0.069  -0.866    0.395
F_lambda1:L_alpha1    0.003      0.010   0.270    0.789</code></pre>
<p><br> <br> <br></p>
</div>
</div>
<div id="second-simulation-with-5-factors" class="section level2">
<h2>Second Simulation with 5 factors</h2>
<div id="simulated-matrix-1" class="section level3">
<h3>Simulated matrix</h3>
<p>For conceptual understanding how both matrix factorizations perform for slightly more complicated matrices, we simulate a 400x10 matrix with the true number of latent factors as 5, with 25% sparse on F and 25% on L. Again, non-sparse factor values follow exp(1/2) distribution (so the signals are positive and also strong enough to be noise). Error matrix has SD = 1/2. L has N(0,1) distribution as above.</p>
<pre><code>[1] &quot;True (avg) sparsity on F: 0.34&quot;</code></pre>
<pre><code>[1] &quot;True (avg) sparsity on L: 0.232&quot;</code></pre>
<pre><code>[1] &quot;Dimension of X: 400x10&quot;</code></pre>
<p><br></p>
</div>
<div id="sn-spmf-1" class="section level3">
<h3>sn-spMF</h3>
<p>For this simulation, we choose a vector of factors K (3, 5, 7) and random initial sparsity parameter values for L and F. We run 8 random starts for each factor.</p>
<pre><code>[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.275801 mins
[1] &quot;Final sparsity in Factor matrix =0.166666666666667; Final sparsity in L =0.03; &quot;
[1] &quot;Converged at itertaion 5&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 15.00412 secs
[1] &quot;Final sparsity in Factor matrix =0.1; Final sparsity in L =0.0225; &quot;
[1] &quot;Converged at itertaion 3&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 9.658805 secs
[1] &quot;Final sparsity in Factor matrix =0.1; Final sparsity in L =0.000833333333333333; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.031997 mins
[1] &quot;Final sparsity in Factor matrix =0.0666666666666667; Final sparsity in L =0.0175; &quot;
[1] &quot;Converged at itertaion 3&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 12.40441 secs
[1] &quot;Final sparsity in Factor matrix =0.0333333333333333; Final sparsity in L =0.0391666666666667; &quot;
[1] &quot;Converged at itertaion 5&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 14.59057 secs
[1] &quot;Final sparsity in Factor matrix =0.1; Final sparsity in L =0.00166666666666667; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.026087 mins
[1] &quot;Final sparsity in Factor matrix =0.1; Final sparsity in L =0.0266666666666667; &quot;
[1] &quot;Converged at itertaion 16&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 42.45616 secs
[1] &quot;Final sparsity in Factor matrix =0.1; Final sparsity in L =0.0266666666666667; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.269371 mins
[1] &quot;Final sparsity in Factor matrix =0.22; Final sparsity in L =0.038; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.291026 mins
[1] &quot;Final sparsity in Factor matrix =0.24; Final sparsity in L =0.047; &quot;
[1] &quot;Converged at itertaion 5&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 18.55691 secs
[1] &quot;Final sparsity in Factor matrix =0.06; Final sparsity in L =0.0035; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.293067 mins
[1] &quot;Final sparsity in Factor matrix =0.24; Final sparsity in L =0.0245; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.286977 mins
[1] &quot;Final sparsity in Factor matrix =0.28; Final sparsity in L =0.055; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.229889 mins
[1] &quot;Final sparsity in Factor matrix =0.32; Final sparsity in L =0.0585; &quot;
[1] &quot;Converged at itertaion 9&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 32.2558 secs
[1] &quot;Final sparsity in Factor matrix =0.08; Final sparsity in L =0.005; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 2.337966 mins
[1] &quot;Final sparsity in Factor matrix =0.26; Final sparsity in L =0.0705; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 5.440764 mins
[1] &quot;Final sparsity in Factor matrix =0.328571428571429; Final sparsity in L =0.0342857142857143; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.183744 mins
[1] &quot;Final sparsity in Factor matrix =0.328571428571429; Final sparsity in L =0.0335714285714286; &quot;
[1] &quot;Converged at itertaion 11&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 55.24979 secs
[1] &quot;Final sparsity in Factor matrix =0.0571428571428571; Final sparsity in L =0.0360714285714286; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 4.846975 mins
[1] &quot;Final sparsity in Factor matrix =0.385714285714286; Final sparsity in L =0.105357142857143; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 3.201288 mins
[1] &quot;Final sparsity in Factor matrix =0.442857142857143; Final sparsity in L =0.0457142857142857; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 6.172327 mins
[1] &quot;Final sparsity in Factor matrix =0.314285714285714; Final sparsity in L =0.1725; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 4.235493 mins
[1] &quot;Final sparsity in Factor matrix =0.357142857142857; Final sparsity in L =0.0310714285714286; &quot;
[1] &quot;Reached maximum iteration.&quot;
[1] &quot;Total time used for updating&quot;
Time difference of 4.322816 mins
[1] &quot;Final sparsity in Factor matrix =0.371428571428571; Final sparsity in L =0.1025; &quot;</code></pre>
<p>Next, we select optimal K based on which K maximizes the cophonet correlation of the “consensus”&quot; matrix. Per Yuan He’s section “Model Selection”, this K shown empirically to produce the stable factor matrices. Next, we select optimal sparsity values for L and F based on lowest correlation between factors.</p>
<p>We use mean absolute pairwise correlation for selecting optimal sparsity parameters. I decided not to use Frobenius norm of correlation matrix because this would actually benefit small pairwise correlations and penalize larger pairwise correlations relative to using absolute correlation (because of the squaring?).</p>
<p><br></p>
</div>
<div id="ebmf-with-flashr-1" class="section level3">
<h3>EBMF with flashr</h3>
<p>Next, perform matrix factorization with EBMF. Again, we use non-negative mf proposed by JW. Same setup as with 1-factor above.</p>
<p>This time, however, we use a combination of flash_add_greedy and flash_backfit (instead of just backfit) with starting K’s from 1 to 8. We then select the final run based on which run minimized our objective.</p>
<pre><code>[1] &quot;Example factor loadings output plots from one run&quot;</code></pre>
<p><img src="figure/initial_analysis.Rmd/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /><img src="figure/initial_analysis.Rmd/unnamed-chunk-15-2.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="results-from-both-methods." class="section level3">
<h3>Results from both methods.</h3>
<p>The “optimal” sn-spmf run (which K gave the highest cophenetic correlation) found the final number of factors to be 3. The best ebmf run (which run maximized the objective) found the optimal number of factors to be 5.</p>
<p>This “optimal” run was chosen as per Yuan He’s documentation with regards to choosing K (number of factors) which maximizes the cophenetic correlation (and supposedly indicates a more stable factor matrix). The sparsity parameters were then selected to minimize pairwise correlation among factors (interpreted as mean absolute pairwise correlation).</p>
<p>If the sn-spmf found K = 5 instead to be optimal, then the best run would have an RRMSE of 0.095, with sparsity levels on F and L of 0.22 and 0.038.</p>
<p>Neither results would have been better than the best ebmf run (highest objective), which had an RRMSE of 0.094. This is still 0.001 lower than the optimal sn-spmf run based on 5 factors, and half as large as RRMSE of the “optimal” sn-spmf run based on 3 factors (definitely significant). Note: the best ebmf run also has the lowest RRMSE. <br></p>
<p>When measured by time, ebmf clearly outpaced sn-spmf and scales much better for larger matrices. Each run of sn-spmf took an average time of 136.957 seconds, more than 5 times as long despite setting the max number of iterations to be the same between the two simulations. Thus, sn-spmf does not seem to scale linearly. Also, to have enough data to use cophenetic correlation to select optimal K, each choice of initial K was paired with 8 random starts on initial sparsity parameters on L and F. So total time required was 54.783 minutes.</p>
<p>Meanwhile, flash_backfit took 2.265 minutes in total, which is much faster. Additionally, flash_backfit seems to scale more linearly with matrix size, taking approximately twice as long for 400x10 matrices as 200x10 matrices.</p>
<pre><code>[1] &quot;&#39;Optimal&#39; selected run results, sn-spmf&quot;</code></pre>
<pre><code>  Run K_start K_final F_lambda1 L_alpha1 F_sparse_final L_sparse_final
3   3       3       3     3.399     0.02            0.1   0.0008333333
   Time Error
3 9.659 0.217</code></pre>
<pre><code>[1] &quot;&quot;</code></pre>
<pre><code>[1] &quot;Best sn-spmf run results based on true K = 5, sn-spmf&quot;</code></pre>
<pre><code>  Run K_start K_final F_lambda1 L_alpha1 F_sparse_final L_sparse_final
9   1       5       5     0.638    0.556           0.22          0.038
     Time Error
9 136.162 0.095</code></pre>
<pre><code>[1] &quot;&quot;</code></pre>
<pre><code>[1] &quot;All flash_backfit runs&quot;</code></pre>
<pre><code>  K_start Time_Taken Num_Factors Objective RRMSE
1       1      6.159           3 -8256.818 0.219
2       2      8.872           3 -8256.981 0.219
3       3     19.192           4 -7912.047 0.151
4       4     12.534           3 -8263.422 0.219
5       5     29.136           5 -7701.152 0.094
6       6     13.156           3 -8256.984 0.219
7       7     31.012           5 -7536.122 0.094
8       8     15.842           3 -8256.987 0.219
  K_start Time_Taken Num_Factors Objective RRMSE
1       1      6.159           3 -8256.818 0.219
2       2      8.872           3 -8256.981 0.219
3       3     19.192           4 -7912.047 0.151
4       4     12.534           3 -8263.422 0.219
5       5     29.136           5 -7701.152 0.094
6       6     13.156           3 -8256.984 0.219
7       7     31.012           5 -7536.122 0.094
8       8     15.842           3 -8256.987 0.219</code></pre>
<pre><code>[1] &quot;&quot;</code></pre>
<pre><code>[1] &quot;Time taken for flash_backfit&quot;</code></pre>
<pre><code>  K_start Time_Taken Num_Factors Objective RRMSE
7       7     31.012           5 -7536.122 0.094
  K_start Time_Taken Num_Factors Objective RRMSE
7       7     31.012           5 -7536.122 0.094</code></pre>
<p><br></p>
<p>In case the optimal number of factors selected by sn-spmf and ebmf do not match anymore, we can still compare the histograms of values of their respective Factor matrices. Histogram of F from ebmf shows better sparsity (initial spike in freqeuency of elements near 0). Histogram of F from sn-spmf doesn’t show this trend as strongly.</p>
<p><img src="figure/initial_analysis.Rmd/unnamed-chunk-18-1.png" width="672" style="display: block; margin: auto;" /><img src="figure/initial_analysis.Rmd/unnamed-chunk-18-2.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="questionspotential-next-steps" class="section level2">
<h2>Questions/Potential Next Steps</h2>
<ul>
<li>How to measure significance in RRMSE differences between runs? Is it sort of like a normalized % error from the truth?</li>
<li>Sparsity on L <span class="math inline">\(\alpha_1\)</span> needs to be bounded by 1 above (anything above 1 and it fails becuase L becomes empty/0 matrix). Why this fails? Does it have to do with penalized? Why very large sparsity on F does not cause run to fail? <br></li>
<li>How to change prior dists in flash to include distributions with mixed point estimate densities (i.e. dense at 0 dists)? <br></li>
<li>What if signal not as strong? Try exp(2)? <br></li>
<li>Change error term to be bigger as well, i.e. as big as signal? Try SD = 2? <br></li>
<li>What if different sparsities on each factor (column) in F? i.e. some are global factors, others more localized. How would either handle? <br></li>
<li>Change back to minimizing Frobenius norm of corr matrix. Will it change results much? <br></li>
<li>When running ebmf with more factors, sometimes get error message – “Warning in estimate_mixprop(data, g, prior, optmethod = optmethod, control = control, : Optimization failed to converge…” –&gt; look into WHY <br></li>
</ul>
<p><br></p>
</div>
<div id="playgroundmisc" class="section level2">
<h2>Playground/Misc</h2>
<div id="backfit-and-plot-fitted-with-actual" class="section level3">
<h3>Backfit and plot fitted with actual</h3>
<p><br></p>
</div>
<div id="greedy-and-plot-fitted-with-actual.-ebnm_ash-has-a-better-lower-objective-though-fairly-small.-why" class="section level3">
<h3>Greedy and plot fitted with actual. “ebnm_ash” has a better (lower) objective, though fairly small. Why?</h3>
<p><br></p>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.5.1 (2018-07-02)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 18362)

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] plyr_1.8.4       NNLM_0.4.3       ebnm_0.1-24      devtools_2.2.1  
 [5] usethis_1.5.0    flashr_0.6-6     forcats_0.3.0    stringr_1.4.0   
 [9] dplyr_0.8.0.1    purrr_0.3.2      readr_1.1.1      tidyr_0.8.1     
[13] tibble_2.1.3     ggplot2_3.2.1    tidyverse_1.2.1  optparse_1.6.4  
[17] penalized_0.9-51 survival_2.42-3  knitr_1.20      

loaded via a namespace (and not attached):
 [1] httr_1.3.1        pkgload_1.0.2     jsonlite_1.5     
 [4] splines_3.5.1     foreach_1.4.7     modelr_0.1.2     
 [7] assertthat_0.2.1  mixsqp_0.2-2      cellranger_1.1.0 
[10] remotes_2.1.0     yaml_2.2.0        sessioninfo_1.1.1
[13] pillar_1.4.2      backports_1.1.5   lattice_0.20-35  
[16] glue_1.3.1        digest_0.6.21     rvest_0.3.2      
[19] colorspace_1.4-1  htmltools_0.3.6   Matrix_1.2-14    
[22] pkgconfig_2.0.3   broom_0.5.0       haven_1.1.2      
[25] scales_1.0.0      processx_3.4.1    whisker_0.3-2    
[28] getopt_1.20.3     git2r_0.25.2      ellipsis_0.3.0   
[31] withr_2.1.2       ashr_2.2-39       lazyeval_0.2.2   
[34] cli_1.1.0         magrittr_1.5      crayon_1.3.4     
[37] readxl_1.1.0      ps_1.2.0          memoise_1.1.0    
[40] evaluate_0.11     fs_1.2.6          doParallel_1.0.15
[43] nlme_3.1-137      MASS_7.3-50       xml2_1.2.0       
[46] truncnorm_1.0-8   pkgbuild_1.0.3    prettyunits_1.0.2
[49] tools_3.5.1       hms_0.4.2         softImpute_1.4   
[52] munsell_0.5.0     callr_3.3.2       compiler_3.5.1   
[55] rlang_0.4.0       grid_3.5.1        iterators_1.0.12 
[58] rstudioapi_0.8    labeling_0.3      rmarkdown_1.10   
[61] testthat_2.2.1    gtable_0.3.0      codetools_0.2-15 
[64] reshape2_1.4.3    R6_2.4.0          lubridate_1.7.4  
[67] workflowr_1.4.0   rprojroot_1.3-2   desc_1.2.0       
[70] stringi_1.4.3     pscl_1.5.2        parallel_3.5.1   
[73] SQUAREM_2017.10-1 Rcpp_1.0.2        tidyselect_0.2.5 </code></pre>
</div>
</div>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
